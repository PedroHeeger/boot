# Formação Cybersec - Labs Módulo 3   <img src="../0-aux/logo_boot.png" alt="boot_035" width="auto" height="45">

### Repository: [boot](../../../../)   
### Platform: <a href="../../../">vnw   <img src="https://github.com/PedroHeeger/my_tech_journey/blob/main/platforms/img/vnw.jpeg" alt="vnw" width="auto" height="25"></a>
### Software/Subject: <a href="../../">cybersecurity   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/content/cybersecurity.jpg" alt="cybersecurity" width="auto" height="25"></a>
### Bootcamp: <a href="../">boot_035 (Formação Cybersec)   <img src="../0-aux/logo_boot.png" alt="boot_035" width="auto" height="25"></a>
### Module: 3. Ethical Hacking (Red Team)

#### <a href="./README.md">Teoria</a>

---

Esta pasta refere-se aos laboratórios do módulo 3 **Ethical Hacking (Red Team)** do bootcamp [**Formação Cybersec**](../).

### Theme:
- Cybersecurity

### Used Tools:
- Operating System (OS): 
  - Linux   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg" alt="linux" width="auto" height="25">
  - Windows 11   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/windows11.png" alt="windows11" width="auto" height="25">
- Linux Distribution:
  - Ubuntu   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/ubuntu/ubuntu-plain.svg" alt="ubuntu" width="auto" height="25">
- Cloud:
  - AWS   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/amazonwebservices/amazonwebservices-original-wordmark.svg" alt="aws" width="auto" height="25">
- Cloud Services:
  - Amazon Elastic Compute Cloud (EC2)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_ec2.svg" alt="aws_ec2" width="auto" height="25">
  - Google Drive   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/google_drive.png" alt="google_drive" width="auto" height="25">
- Containerization: 
  - Docker   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg" alt="docker" width="auto" height="25">
  - Docker Compose   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/docker_compose.png" alt="docker_compose" width="auto" height="25">
  - Docker Playground; Play With Docker (PWD)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/sites/docker_playground.jpg" alt="docker_playground" width="auto" height="25">
- Build Automation:
  - Make   <img src="" alt="make" width="auto" height="25">
- Language:
  - HTML   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/html5/html5-original.svg" alt="html" width="auto" height="25">
  - Markdown   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/markdown/markdown-original.svg" alt="markdown" width="auto" height="25">
- Integrated Development Environment (IDE) and Text Editor:
  - Nano   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/nano.png" alt="nano" width="auto" height="25">
  - Vi   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/vi.png" alt="vi" width="auto" height="25">
  - VI iMproved (Vim)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vim/vim-original.svg" alt="vim" width="auto" height="25">
  - Visual Studio Code (VS Code)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vscode/vscode-original.svg" alt="vscode" width="auto" height="25">
- Versioning: 
  - Git   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg" alt="git" width="auto" height="25">
- Repository:
  - GitHub   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg" alt="github" width="auto" height="25">
- Command Line Interpreter (CLI):
  - AWS Command Line Interface (CLI)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_cli.svg" alt="aws_cli" width="auto" height="25">
  - Bash e Sh   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bash/bash-original.svg" alt="bash_sh" width="auto" height="25">
- Tools:
  - Advanced Package Tool (Apt)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/apt.png" alt="apt" width="auto" height="25">
  - Advanced Package Tool (Apt-Get)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/apt-get.jpg" alt="apt-get" width="auto" height="25">
  - Curl   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/curl.png" alt="curl" width="auto" height="25">
- Network:
  - netstat   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/netstat.webp" alt="netstat" width="auto" height="25">
  - Nmap   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/nmap.png" alt="nmap" width="auto" height="25">
  - OpenSSH   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/openssh.png" alt="openssh" width="auto" height="25">
  - OWASP Juice Shop   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/sites/owasp_juice_shop.png" alt="owasp_juice_shop" width="auto" height="25">
  - OWASP ModSecurity Core Rule Set (OWASP ModSecurity CRS)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/owasp_modesecurity_crs.png" alt="owasp_modesecurity_crs" width="auto" height="25">
  - Uncomplicated Firewall (UFW)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/ufw.webp" alt="ufw" width="auto" height="25">
- Remote Desktop:
  - RealVNC Viewer   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/realvnc.png" alt="realvnc_viewer" width="auto" height="25">
- Cibersecurity:
  - Amass   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/amass.png" alt="amass" width="auto" height="25">
  - dnsx   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/dnsx.png" alt="dnsx" width="auto" height="25">
  - Gitleaks   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/gitleaks.png" alt="gitleaks" width="auto" height="25">
  - Hakrawler   <img src="" alt="hakrawler" width="auto" height="25">
  - httpx   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/httpx.png" alt="httpx" width="auto" height="25">
  - Kali Linux   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/kali_linux.png" alt="kali_linux" width="auto" height="25">
  - Nuclei   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/nuclei.png" alt="nuclei" width="auto" height="25">
  - Sqlmap   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/sqlmap.png" alt="sqlmap" width="auto" height="25">
  - Subfinder   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/subfinder.svg" alt="subfinder" width="auto" height="25">
  - Trivy   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/trivy.png" alt="trivy" width="auto" height="25">
- SysAdm:
  - Xfce   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/xfce.svg" alt="xfce" width="auto" height="25">

---

### Bootcamp Module 3 Structure:
3. <a name="item3">Módulo 3: Ethical Hacking (Red Team)<br>
  3.1 <a href="#item3.1">Pentest: Metodologia & Regras de Engajamento</a><br>
  3.2 <a href="#item3.2">Lab 2: Descobrindo os Segredos dos Servidores Web</a><br>
  3.3 <a href="#item3.3">Firewall & ACL</a><br>
  3.4 <a href="#item3.4">IDS e IPS</a><br>
  3.5 <a href="#item3.5">Monitoramento de Logs</a><br>
  3.6 <a href="#item3.6">Patch Management</a><br>
  3.7 <a href="#item3.7">Cloud Security</a><br>
  3.8 <a href="#item3.8">IAM e Permissionamento</a><br>
  3.9 <a href="#item3.9">Container Security Docker Bench & Trivy</a><br>
  3.10 <a href="#item3.10">NIST & Resposta a Incidentes</a><br>

---

### Objective:
Desenvolver a mentalidade ofensiva necessária para simular ataques reais e transformar descobertas em evidências técnicas e recomendações acionáveis, por meio da aplicação de metodologias de pentest, exploração de vulnerabilidades com ferramentas como **Metasploit** e **Burp Suite**, técnicas de quebra de credenciais, escalonamento de privilégios em **Linux** e **Windows**, movimentação lateral, evasão de controles de defesa e elaboração de relatórios técnicos profissionais.

### Folder Structure:
- [README.md](./README.md): Documento escrito em **Markdown** descrevendo todo conteúdo teórico realizado neste módulo.
- [labs.md](./labs.md): Este documento de README, escrito em **Markdown**, descrevendo todos os laboratórios realizados neste módulo.

### Development:
Em cibersegurança, é prática comum e recomendada realizar laboratórios em ambientes controlados e seguros. Por isso, ferramentas como **Docker** ou máquinas virtuais são utilizadas para simular ambientes reais, protegendo a infraestrutura local, uma vez que o uso de ferramentas de ataque ou alterações em configurações de rede podem comprometer a integridade do sistema. O ambiente de laboratório é estruturado em duas partes: a primeira envolve a instalação dos softwares que virtualização um ambiente, como **WSL2**, **Docker** ou **VM VirtualBox**, além de ferramentas de suporte essenciais, como editores de código (**Visual Studio Code (VS Code)**) e sistemas de versionamento (**Git**).  

Neste curso, a virtualização do ambiente foi realizada principalmente com **Docker** em conjunto com **WSL**. No entanto, para a maioria dos labs utilizei o **Docker** em instâncias do **Amazon Elastic Compute Cloud (EC2)**, na nuvem da **Amazon Web Services (AWS)**, como alternativa de execução. Em pequenos casos, para realizar testes, a plataforma **Play With Docker (PWD)** também foi utilizada.

A criação da instância EC2 foi automatizada por meio do script [`ec2Instance.ps1`](../environment/ec2Instance.ps1), desenvolvido em **Windows PowerShell** utilizando comandos da **AWS Command Line Interface (CLI)**. O script está localizado na pasta [`environment`](../environment/) deste curso, pois foi o mesmo para todos os módulos. A instância foi provisionada com a imagem `ami-020cba7c55df1f615`, baseada no sistema operacional **Linux Ubuntu**, associada a um volume do **Amazon Elastic Block Store (EBS)** de `8 GB`, do tipo `gp` (General Purpose). O tipo de instância utilizado foi o `t3.medium`, com 2 vCPUs e 4 GB de memória. Para acesso, foi utilizado o par de chaves `keyPairUniversal`, previamente existente na conta da **AWS**, e o grupo de segurança atribuído à instância foi o `default` da zona de disponibilidade `us-east-1a` (Norte da Virgínia). Um script de *user data* foi utilizado para automatizar a instalação do **Git** e do **Docker** durante o processo de inicialização.

O acesso à instância podia ser feito tanto pelo console da **AWS** quanto por meio de conexão SSH utilizando o **OpenSSH** no **Windows PowerShell** da máquina local. Neste último caso, era necessário informar o caminho do arquivo de chave privada, o nome do usuário do sistema e o IP ou DNS público da instância. Um exemplo de comando seria: `ssh -i "G:/Meu Drive/4_PROJ/scripts/aws/.default/secrets/awsKeyPair/universal/keyPairUniversal.pem" ubuntu@54.160.249.118`. Além disso, o Security Group associado à instância precisava conter uma regra de entrada liberando a porta `22` para o IP público da máquina física, a fim de permitir o estabelecimento da conexão SSH. As máquinas virtuais do **Play With Docker (PWD)** também foram acessadas via SSH. Nesse caso, o próprio ambiente fornecia o comando necessário para a conexão, que podia ser executado diretamente no **Windows PowerShell**, sem necessidade de autenticação com chave privada ou senha.

A segunda parte do ambiente de laboratório consiste na construção do ambiente simulado propriamente dito. No caso do **Docker**, isso envolve a criação de containers, redes e volumes, realizada por meio de dois tipos principais de arquivos. O primeiro é o `docker-compose.yml`, que define de forma serial toda a estrutura a ser criada: quais e quantos containers, suas configurações, as imagens que irão utilizar e os volumes e redes que serão estabelecidos. O segundo tipo de arquivo, que pode existir mais de um por laboratório, são os `Dockerfile`, responsáveis por criar imagens específicas para cada container. Essas imagens podem ser enviadas a repositórios de imagens **Docker**, como o **DockerHub**, ou referenciadas diretamente no arquivo do **Docker Compose**.

Todos esses arquivos eram preparados pelo instrutor do curso e disponibilizados no [repositório](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/) do curso no perfil da plataforma **Vai na Web** no **GitHub**. O repositório era organizado pelos três módulos do curso, com pastas correspondentes a cada laboratório. Geralmente, o número do laboratório coincidia com o número da aula, embora nem todos os labs seguissem essa sequência e nem todas as aulas tivessem laboratórios. Em cada pasta de laboratório, além dos arquivos `docker-compose.yml` e `Dockerfile`, podiam existir arquivos complementares, como scripts, textos ou documentos **Markdown**, contendo informações relevantes ou conteúdos necessários para a execução do lab. Durante a realização de cada lab, além de executar os exercícios, foi feita uma explicação detalhada sobre a construção dos arquivos de **Docker Compose**,  **Docker** e as dependências utilizadas, evidenciando como o ambiente foi estruturado.

Para executar os arquivos e iniciar o ambiente de laboratório, a sequência de comandos utilizada era a seguinte:
- `git clone https://github.com/Kensei-CyberSec-Lab/formacao-cybersec.git`: Clonagem do repositório do bootcamp para o ambiente local, seja no **WSL**, em máquinas virtuais ou, como no meu caso, em instâncias **Amazon EC2**.
- `cd formacao-cybersec/modulo3-ethical-hacking/lab_1`: Navegação até o diretório do laboratório a ser executado. Para outros labs, bastava alterar as duas últimas pastas do caminho para o módulo e lab correspondentes.
- `docker compose up -d`: Inicialização do ambiente com **Docker Compose**. Este comando devia ser executado na pasta onde o arquivo `docker-compose.yml` estava localizado.
- `docker ps` e `docker network ls`: Verificação dos containers ativos e das redes existentes no ambiente.
- `docker exec -it kali /bin/bash`: Acesso a um container em execução. Bastava substituir `kali` pelo nome do container que desejava-se acessar.
- `docker compose down`: Encerramento do ambiente. Assim como na inicialização, este comando devia ser executado na pasta onde se encontrava o arquivo `docker-compose.yml`.
- `docker system prune -f`: Remoção containers parados, redes não usadas, imagens dangling (imagens sem tags) e caches de build.
- `docker system prune -a`: Remoção containers parados, redes não usadas, imagens dangling (imagens sem tags) e caches de build, além de remoção de todas as imagens não usadas por containers.

Os laboratórios de cibersegurança são organizados com máquinas de ataque, geralmente uma **Kali Linux**, e máquinas alvo ou de defesa, que são os sistemas onde os ataques são realizados. Pode haver múltiplas máquinas de cada tipo, sendo comum que algumas máquinas de defesa sejam propositalmente vulneráveis, incluindo aplicações web criadas para testes. Como o ambiente é simulado via **Docker**, termos como servidor, máquina, container ou host frequentemente se referem aos containers que representam as máquinas simuladas. Além disso, é importante ter em mente que existiam outras duas camadas no ambiente: a máquina física, no caso meu computador pessoal **Windows**, e a máquina virtual fornecida pela **AWS** ou pelo **Play With Docker (PWD)**, que hospedava e executava os containers do **Docker**.

Outra parte importante dos laboratórios foram os *Capture The Flag (CTF)*, desafios técnicos amplamente utilizados na área de cibersegurança para desenvolver e validar conhecimento prático. Cada desafio apresenta um cenário específico — como exploração de vulnerabilidades, análise de tráfego, OSINT ou engenharia reversa — e exige que o participante realize uma ação ou resolva um problema para obter a flag. A flag não se limita a uma string ou código a ser encontrado, ela representa a prova de que o objetivo do desafio foi cumprido com sucesso. Ao longo dos laboratórios do curso, diversos CTFs foram incorporados ao ambiente simulado, servindo como etapas práticas de validação do conteúdo e permitindo aplicar, de forma objetiva, os conceitos aprendidos. Dessa forma, os laboratórios não apenas simularam cenários reais de ataque e defesa, como também proporcionaram desafios progressivos que reforçaram o raciocínio lógico, a análise técnica e a consolidação do conhecimento.

<a name="item3.1"><h4>3.1 Pentest: Metodologia & Regras de Engajamento</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_1)

Obs.: Laboratório registrado como 1, documento como 1 e referente a aula 1.

<details><summary><strong>Ambiente de Laboratório</strong></summary>
  <ul>
    <li><details><summary><strong>Docker Compose</strong></summary>
        <ul>
          <li><details><summary><strong>services:</strong></summary>
            <ul>
              <li><details><summary><strong>kali:</strong></summary>
                <ul>
                  <li><strong>build:</strong>
                    <ul>
                      <li><code>context: .</code>: Contexto da build é o diretório atual.</li>
                      <li><code>dockerfile: kali.Dockerfile</code>: Usa o arquivo <code>kali.Dockerfile</code> para construir a imagem.</li>
                    </ul>
                  </li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_kali</code>.</li>
                  <li><code>tty: true</code>: Permite alocar um terminal interativo para o container.</li>
                  <li><code>stdin_open: true</code>: Mantém o STDIN aberto para uso com <code>docker exec -it</code>.</li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./labs:/home/kali/labs</code>: Monta o diretório local <code>./labs</code> dentro do container em <code>/home/kali/labs</code>, facilitando desenvolvimento e uso de scripts.</li>
                    </ul>
                  </li>
                  <li><strong>command:</strong> <code>/bin/bash</code>: Inicia o shell Bash, mantendo o container pronto para uso.</li>
                </ul>
              </details></li>
              <li><details><summary><strong>spiderfoot:</strong></summary>
                <ul>
                  <li><strong>build:</strong>
                    <ul>
                      <li><code>context: .</code>: Define o diretório atual como contexto da build.</li>
                      <li><code>dockerfile: Dockerfile.spiderfoot</code>: Usa o Dockerfile dedicado do SpiderFoot para construir a imagem.</li>
                    </ul>
                  </li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_spiderfoot</code>.</li>
                  <li><strong>ports:</strong>
                    <ul>
                      <li><code>"5001:5001"</code>: Expõe a interface web do SpiderFoot na porta 5001 do host.</li>
                    </ul>
                  </li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./spiderfoot-data:/root/.spiderfoot</code>: Monta o diretório local <code>./spiderfoot-data</code> dentro do container em <code>/root/.spiderfoot</code>, garantindo que configurações, resultados e dados do SpiderFoot sejam persistidos entre reinicializações do container.</li>
                    </ul>
                  </li>
                </ul>
              </details></li>
              <li><details><summary><strong>neo4j:</strong></summary>
                <ul>
                  <li><strong>image:</strong> Usa a imagem oficial <code>neo4j:4.4</code>.</li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_neo4j</code>.</li>
                  <li><strong>environment:</strong>
                    <ul>
                      <li><code>NEO4J_AUTH=neo4j/test</code>: Define usuário e senha iniciais (<code>neo4j</code> / <code>test</code>).</li>
                      <li><code>NEO4J_dbms_memory_heap_initial__size=512m</code>: Define o tamanho inicial do heap de memória do Neo4j como 512 MB.</li>
                      <li><code>NEO4J_dbms_memory_heap_max__size=1g</code>: Define o tamanho máximo do heap de memória do Neo4j como 1 GB.</li>
                    </ul>
                  </li>
                  <li><strong>ports:</strong>
                    <ul>
                      <li><code>"7474:7474"</code>: Mapeia a porta 7474 do container para a porta 7474 do host, permitindo acesso à interface web do Neo4j via navegador.</li>
                      <li><code>"7687:7687"</code>: Mapeia a porta 7687 do container para a porta 7687 do host, utilizada pelo Bolt protocol para conexão de drivers e aplicações ao Neo4j.</li>
                    </ul>
                  </li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./neo4j-data:/data</code>: Monta o diretório local <code>./neo4j-data</code> dentro do container em <code>/data</code>, garantindo que o banco de dados Neo4j seja persistido no host entre reinicializações do container.</li>
                    </ul>
                  </li>
                </ul>
              </details></li>
            </ul>
          </details></li>
          <li><details><summary><strong>networks:</strong></summary>
            <ul>
              <li><code>default:</code>
                <ul>
                  <li><strong>name:</strong> <code>kensei_lab_net</code>: Nomeia a rede padrão usada pelos serviços, garantindo comunicação entre containers dentro dessa rede.</li>
                </ul>
              </li>
            </ul>
          </details></li>
        </ul>
      </details></li>
    <li><details><summary><strong>Dockerfile</strong></summary>
      <ul>
        <li><details><summary><strong>Dockerfile.kali</strong></summary>
          <ul>
            <li><code>FROM kalilinux/kali-rolling</code>: Imagem base Kali Rolling.</li>
            <li><strong>RUN:</strong> Executa a instalação de ferramentas essenciais e limpa o cache para reduzir o tamanho da imagem:
              <ul>
                <li><code>apt-get update</code>: Atualiza a lista de pacotes disponíveis.</li>
                <li><code>apt-get install -y git curl jq python3-pip build-essential golang-go amass</code>: Instala ferramentas essenciais de desenvolvimento, rede e pentest.</li>
                <li><code>apt-get clean && rm -rf /var/lib/apt/lists/*</code>: Remove arquivos temporários e limpa cache do apt.</li>
              </ul>
            </li>
            <li><code>ENV GOPATH=/root/go</code>: Define a variável <code>GOPATH</code> apontando para <code>/root/go</code>, diretório onde o Go instalará pacotes e binários do usuário.</li>
            <li><code>ENV PATH=$PATH:/root/go/bin</code>: Adiciona <code>/root/go/bin</code> ao <code>PATH</code>, permitindo executar ferramentas instaladas via <code>go install</code>.</li>
            <li><code>RUN mkdir -p $GOPATH</code>: Cria diretório do GOPATH.</li>
            <li><strong>RUN:</strong> Instala ferramentas Go no <code>GOPATH</code> utilizando <code>go install ...@latest</code> e disponibiliza os binários em <code>/root/go/bin</code>:
              <ul>
                <li><code>go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest</code>: Instala o <strong>subfinder</strong>, usado para descoberta de subdomínios.</li>
                <li><code>go install github.com/projectdiscovery/httpx/cmd/httpx@latest</code>: Instala o <strong>httpx</strong>, utilizado para verificação e coleta de informações HTTP (status, headers, títulos, etc.).</li>
                <li><code>go install github.com/projectdiscovery/nuclei/v2/cmd/nuclei@latest</code>: Instala o <strong>nuclei</strong>, motor de scan de vulnerabilidades baseado em templates.</li>
                <li><code>go install github.com/hakluke/hakrawler@latest</code>: Instala o <strong>hakrawler</strong>, um crawler rápido para descoberta de endpoints e conteúdo web.</li>
                <li><code>go install github.com/zricethezav/gitleaks/v8@latest</code>: Instala o <strong>gitleaks</strong>, ferramenta para detecção de segredos e chaves em repositórios.</li>
              </ul>
            </li>
            <li><code>COPY scripts /home/kali/scripts</code>: Copia scripts locais para dentro do container.</li>
            <li><code>WORKDIR /home/kali</code>: Define diretório de trabalho.</li>
            <li><code>CMD ["/bin/bash"]</code>: Inicia o shell Bash.</li>
          </ul>
        </details></li>
        <li><details><summary><strong>Dockerfile.spiderfoot</strong></summary>
          <ul>
            <li><code>FROM python:3.10-slim</code>: Usa a imagem base Python 3.10 slim como ambiente inicial.</li>
            <li><strong>RUN</strong>:
              <ul>
                <li><code>apt-get update</code>: Atualiza a lista de pacotes.</li>
                <li><code>apt-get install -y --no-install-recommends git build-essential python3-dev libyaml-dev curl ca-certificates gcc g++ make libffi-dev libssl-dev</code>: Instala dependências do sistema necessárias para compilar e executar as dependências Python do SpiderFoot.</li>
                <li><code>rm -rf /var/lib/apt/lists/*</code>: Remove listas de pacotes para reduzir o tamanho final da imagem.</li>
              </ul>
            </li>
            <li><code>RUN pip3 install -U pip setuptools wheel cython</code>: Atualiza o gerenciador de pacotes Python e instala ferramentas de build (necessárias para compilar extensões como PyYAML).</li>
            <li><code>WORKDIR /app</code>: Define o diretório de trabalho para os comandos seguintes.</li>
            <li><code>RUN git clone --depth=1 --branch v4.0 https://github.com/smicallef/spiderfoot /app</code>: Clona a versão 4.0 do repositório SpiderFoot diretamente para <code>/app</code>.</li>
            <li><code>RUN sed -i 's/pyyaml>=5.4.1,<6/pyyaml>=6.0/' requirements.txt</code>: Altera a especificação do PyYAML no <code>requirements.txt</code> para uma versão compatível (quando necessário) antes da instalação das dependências.</li>
            <li><code>RUN pip3 install -r requirements.txt</code>: Instala as dependências Python do SpiderFoot listadas em <code>requirements.txt</code>.</li>
            <li><code>EXPOSE 5001</code>: Expõe a porta 5001, usada pela interface web do SpiderFoot.</li>
            <li><code>CMD ["python3", "sf.py", "-l", "0.0.0.0:5001"]</code>: Comando padrão para iniciar o SpiderFoot, fazendo-o escutar em todas as interfaces na porta 5001.</li>
          </ul>
        </details></li>
      </ul>
    </details></li>
  </ul>
</details>

Os laboratórios do módulo três, na maioria das vezes, seguiram uma sequência interligada: um processo realizado em um laboratório anterior servia como base para o próximo. Neste módulo, o foco foi o ataque — o objetivo era explorar sistemas em níveis que iam do introdutório ao avançado, permitindo compreender os processos gradualmente. Era terminantemente proibido comprometer sistemas reais; foram utilizados apenas sistemas criados pelo instrutor para fins de aprendizado, com acesso e ações estritamente controlados pelos instrutores. Esses sistemas foram gerenciados na **AWS** e envolveram variados serviços. Na maioria dos labs foi utilizado um único sistema, cujo nível de dificuldade aumentava progressivamente; em outros, existiram sistemas distintos destinados a finalidades específicas.

O primeiro laboratório consistiu em executar OSINT — com footprinting passivo e ativo — em um dos sistemas fornecidos pelo instrutor. O alvo era um sistema de uma empresa fictícia de logística, denominado Acme Corp. O objetivo foi realizar o reconhecimento, descobrir e documentar informações sobre a infraestrutura e realizar uma exploração leve e controlada (footprinting ativo) sem causar impacto ao ambiente. Todo o processo teve caráter investigativo: analisar o sistema e registrar as observações para embasar e orientar as etapas subsequentes em outros laboratórios.

O ambiente **Docker** foi implantado em uma instância **Amazon EC2** na **AWS** e era composto pelos três containers seguintes:
- `kensei_kali`: **Kali Linux** utilizado para ataque e reconhecimento. Continha ferramentas como **Subfinder**, **Httpx**, **Nuclei**, **Hakrawler**, **Gitleaks**, além de softwares como: **Python**, **curl**, **git**, **jq**, **build-essential**, **Golang** e **Amass**.
- `kensei_spiderfoot`: container executando o **SpiderFoot**.
- `kensei_neo4j`: container executando o **Neo4j**.

Antes do início da primeira etapa, o reconhecimento, foi feito o acesso ao container **Kali Linux** a partir da instância remota utilizando o comando `docker exec -it kensei_kali /bin/bash`. Dentro do container, foi criada a pasta de trabalho `acme-corp` com `mkdir -p /home/kali/investigations/acme-corp` e, em seguida, acessado o diretório com `cd /home/kali/investigations/acme-corp`. A maior parte dos comandos executados teve seus outputs salvos em arquivos dentro dessa pasta. Uma pasta idêntica, com a cópia desses arquivos, foi criada na pasta desse módulo, chamada [acme-corp](./acme-corp/).

🔍 Fase 1 — Reconhecimento   
Na Fase 1, o objetivo foi enumerar todos os subdomínios associados ao domínio da empresa Acme Corp (`acme-corp-lab.com`). A partir do container de ataque Kali, foi executado o **Subfinder** com o comando `subfinder -d acme-corp-lab.com -o subdomains.txt`, que coletou os subdomínios e salvou o resultado no arquivo [subdomains.txt](./acme-corp/subdomains.txt), dentro do diretório criado em `/home/kali/investigations/acme-corp`. Em seguida, verificou-se o conteúdo gerado com `cat /home/kali/investigations/acme-corp/subdomains.txt` para confirmar os subdomínios encontrados, conforme ilustrado na imagem 01.

<div align="center"><figure>
    <img src="../0-aux/md3-img01.png" alt="img01"><br>
    <figcaption>Imagem 01.</figcaption>
</figure></div><br>

Um dos subdomínios identificados apresentava o prefixo `old`, indicando tratar-se de um sistema legado — normalmente mais suscetível a vulnerabilidades. Entretanto, o reconhecimento continuou com a execução do **Amass** com tempo limitado usando `timeout 300 amass enum -passive -d acme-corp-lab.com -o amass_results.txt -v || echo "Amass timeout - continuando com Subfinder"`, de modo que quaisquer resultados fossem gravados em [amass_results.txt](./acme-corp/amass_results.txt). Por fim, verificou-se o conteúdo retornado pelo **Amass** com `cat /home/kali/investigations/acme-corp/amass_results.txt`.

```bash
if [ -f amass_results.txt ]; then
    echo "=== RESULTADOS DO AMASS ==="
    cat amass_results.txt
else
    echo "Amass não completou - usando apenas resultados do Subfinder"
fi
```

O **Amass** demorou muito e não trouxe novas descobertas; por isso, foram utilizados apenas os resultados do **Subfinder**. O **Amass** é uma ferramenta valiosa, pois agrega informações de várias fontes. Porém, como o domínio era recente e criado exclusivamente para o laboratório, não havia muita informação pública disponível sobre ele.

Ainda dentro do reconhecimento, o próximo passo foi utilizar o **dnsx** para verificar nos servidores DNS se cada um dos quatro subdomínios descobertos possuía algum registro válido. Registros DNS podem ser do tipo A (IPv4), AAAA (IPv6) ou outros, como CNAME. Encontrar um registro DNS indicava que o domínio estava ativo; caso contrário, o domínio não estava configurado ou registrado em um servidor DNS. Dessa forma, executou-se `cat subdomains.txt | dnsx -resp -silent -o resolved_subdomains.txt`, que consultou cada subdomínio e salvou as entradas resolvidas no arquivo [resolved_subdomains.txt](./acme-corp/resolved_subdomains.txt). Os resultados, conforme imagem 02, mostraram que todos os subdomínios possuíam múltiplos registros do tipo A, indicando que cada subdomínio resolvia para vários endereços IP distintos. Esse cenário sugeria a existência de uma infraestrutura distribuída ou a utilização de balanceamento de carga.

<div align="center"><figure>
    <img src="../0-aux/md3-img02.png" alt="img02"><br>
    <figcaption>Imagem 02.</figcaption>
</figure></div><br>

Na sequência, foi utilizado o **httpx** para realizar requisições HTTP/HTTPS em cada um dos subdomínios descobertos, identificando informações como título da página, tecnologias utilizadas e código de status. O comando executado foi `cat subdomains.txt | httpx -title -tech-detect -status-code -o live_web_services.txt`, e os resultados foram salvos no arquivo [live_web_services.txt](./live_web_services.txt) dentro do diretório `/home/kali/investigations/acme-corp/`. A imagem 03 apresenta os resultados, permitindo identificar o seguinte:
- `www.acme-corp-lab.com`: site estático hospedado no **Amazon S3**, típico do site principal.
- `admin.acme-corp-lab.com`: painel **WordPress**, alvo comum em testes de segurança de interfaces administrativas.
- `old.acme-corp-lab.com`: retornou HTTP 301, indicando um redirecionamento.

#TROCAR AQUI

<div align="center"><figure>
    <img src="../0-aux/md3-img03.png" alt="img03"><br>
    <figcaption>Imagem 03.</figcaption>
</figure></div><br>

🎯 Fase 2 — Descoberta   
A fase de descoberta foi realizada acessando cada domínio no navegador da máquina física **Windows** ou via **Curl** no container de ataque **Kali Linux**. Ao acessar `www.acme-corp-lab.com` em ambos os ambientes, a resposta HTTP foi 200 e o conteúdo exibido correspondia ao domínio raiz `acme-corp-lab.com`. Isso indicava que a distribuição do **Amazon CloudFront** estava configurada para servir tanto o subdomínio `www.acme-corp-lab.com` quanto o domínio raiz `acme-corp-lab.com` a partir da mesma infraestrutura — com o **CloudFront** posicionado na frente do **Amazon S3** que hospedava o site estático — em vez de retornar um redirecionamento HTTP explícito. A imagem 04 mostra o acesso a `www.acme-corp-lab.com` no navegador.

<div align="center"><figure>
    <img src="../0-aux/md3-img04.png" alt="img04"><br>
    <figcaption>Imagem 04.</figcaption>
</figure></div><br>

O domínio era público, pois o laboratório exigia um domínio acessível para que o processo de OSINT pudesse ser realizado por ferramentas de coleta de informações públicas. Os demais subdomínios não estavam expostos publicamente, já que seriam alvos de exploração controlada e, portanto, exigiam regras de acesso restritas. Um dos instrutores foi responsável por autorizar o acesso dos alunos — provavelmente adicionando o endereço IP público de cada aluno a um grupo de segurança com permissão para acessar esses subdomínios.

Após a autorização do IP público da minha instância `Amazon EC2` (host do ambiente `Docker`), acessou-se o subdomínio `admin.acme-corp-lab.com` sem obter resultados relevantes. Em seguida, acessou-se `old.acme-corp-lab.com`, que redirecionou para o IP `3.94.82.59` e exibiu uma página intitulada `Legacy System` com três links de interesse (conforme imagem 05). Antes de abrir os links no navegador, repetiu-se o mesmo procedimento via **Curl** no container de ataque: `curl -L -s http://old.acme-corp-lab.com/ > old_acme_page.html`, que seguiu a cadeia de redirecionamentos até a página final e salvou o conteúdo em [old_acme_page.html](./acme-corp/old_acme_page.html). Como o IP final do redirecionamento já era conhecido, também foi possível obter a mesma página diretamente com `curl -s http://3.94.82.59/ > legacy_page.html`, salvando o mesmo conteúdo HTML no arquivo [legacy_page.html](./acme-corp/legacy_page.html).

<div align="center"><figure>
    <img src="../0-aux/md3-img05.png" alt="img05"><br>
    <figcaption>Imagem 05.</figcaption>
</figure></div><br>

Com os resultados obtidos, foi possível inferir que a página funcionava como um hub central, encaminhando para outros serviços por meio dos três links identificados. Embora os comentários HTML no final da página não fossem visíveis diretamente no navegador, eles apareceram ao salvar o conteúdo com **Curl**. Para extrair esses comentários utilizou-se `awk '/^<!--/,/-->$/' legacy_page.html`, que isolou os blocos de comentários HTML no arquivo, e esses comentários revelaram duas URLs públicas do **Amazon S3**, conforme evidenciado na imagem 06.

<div align="center"><figure>
    <img src="../0-aux/md3-img06.png" alt="img06"><br>
    <figcaption>Imagem 06.</figcaption>
</figure></div><br>

Com os dois comandos a seguir foram baixados os arquivos do S3: `curl -s https://acme-corp-lab-public-files-6nssymq7.s3.us-east-1.amazonaws.com/company_info.txt` e `curl -s https://acme-corp-lab-public-files-6nssymq7.s3.us-east-1.amazonaws.com/employees.csv`. Ambos foram salvos no diretório de trabalho `/home/kali/investigations/acme-corp/`. O arquivo [company_info.txt](./acme-corp/company_info.txt) continha informações sobre a infraestrutura da empresa, endereços de escritórios e servidores de e-mail e DNS. Já o arquivo [employees.csv](./acme-corp/employees.csv) trazia uma lista completa de funcionários com endereços de e-mail e números de telefone. Ou seja, essa empresa estava vazando informações sensíveis.

🔍 Fase 3 — Exploração   
Na fase de exploração, testaram-se os dois subdomínios que não haviam retornado resultados (`admin.acme-corp-lab.com` e `dev.acme-corp-lab.com`) utilizando também o endpoint genérico da **AWS** para buckets com site estático na região `us-east-1` (Norte Virgínia): `s3-website-us-east-1.amazonaws.com`. Nesse cenário, o nome do subdomínio é usado como nome do bucket e o domínio torna-se o endpoint da **AWS**. Os testes foram realizados via **Curl** com `curl -I http://admin.acme-corp-lab.com.s3-website-us-east-1.amazonaws.com` e `curl -I http://dev.acme-corp-lab.com.s3-website-us-east-1.amazonaws.com` (imagem 07), além de verificações pelo navegador na máquina física **Windows** (imagens 09 e 10).

<div align="center"><figure>
    <img src="../0-aux/md3-img07.png" alt="img07"><br>
    <figcaption>Imagem 07.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img08.png" alt="img08"><br>
    <figcaption>Imagem 08.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img09.png" alt="img09"><br>
    <figcaption>Imagem 09.</figcaption>
</figure></div><br>

Como mostrado nas imagens, essas URLs redirecionavam diretamente para endereços IP — algo comum na infraestrutura da **AWS**. As páginas desses IPs eram exibidas normalmente no navegador, mas para acessá-las via **Curl** foi necessário executar `curl -I http://54.152.245.201:80/` e `curl -s http://34.207.53.34:3000/ | head -5`. Os IPs acessados foram `54.152.245.201:80`, que correspondia a um serviço administrativo, e `34.207.53.34:3000`, relacionado a uma API chamada Dev. Lembra dos três links na página do subdomínio `old.acme-corp-lab.com`? Os dois últimos (`Admin Panel` e `API`) redirecionavam exatamente para esses serviços. O serviço administrativo exibiu informações sobre **WordPress**, enquanto a API apresentou quatro endpoints: `/api/health`, `/api/system-info`, `/api/users` e `/api/config`.

Na segurança da informação, APIs são sempre alvos valiosos. Por isso, os quatro endpoints identificados foram testados via **Curl** com os comandos: `curl -s http://34.207.53.34:3000/api/health`, `curl -s http://34.207.53.34:3000/api/system-info`, `curl -s http://34.207.53.34:3000/api/users` e `curl -s http://34.207.53.34:3000/api/config`, conforme imagem 10. Os mesmos acessos também foram realizados pelo navegador da máquina física, e as imagens 11 a 14 mostram os resultados de cada endpoint.

<div align="center"><figure>
    <img src="../0-aux/md3-img10.png" alt="img10"><br>
    <figcaption>Imagem 10.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img11.png" alt="img11"><br>
    <figcaption>Imagem 11.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img12.png" alt="img12"><br>
    <figcaption>Imagem 12.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img13.png" alt="img13"><br>
    <figcaption>Imagem 13.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img14.png" alt="img14"><br>
    <figcaption>Imagem 14.</figcaption>
</figure></div><br>

O endpoint `/api/system-info` revelou dados sensíveis de configuração, incluindo a porta `3306` (indicando um **MySQL**), nome de usuário, senha e nome do banco, além de chaves de API para **Stripe**, **Sendgrid**, **AWS** e **GitHub**. Em um ambiente de produção, essas informações poderiam ser utilizadas para comprometer a base de dados e outras partes da infraestrutura, permitindo exfiltração de dados, execução de comandos remotos via serviços em nuvem ou uso indevido das integrações com terceiros.

🔍 Fase 4 — Enumeração   
Na fase de enumeração utilizou-se o IP do serviço administrativo (`54.152.245.201:80`), previamente identificado como um **WordPress**, com o objetivo de mapear diretórios e descobrir caminhos interessantes. Foi criada uma wordlist personalizada com `echo -e 'admin\napi\nbackup\nconfig\ndev\ngit\nlogin\nphpinfo\nphpmyadmin\ntest\nwww\nwp-admin\nwp-content\nwp-includes\nuploads\nfiles\nimages\ncss\njs\nassets' > /home/kali/investigations/acme-corp/custom_wordlist.txt`, que gerou o arquivo [custom_wordlist.txt](./acme-corp/custom_wordlist.txt) contendo nomes de diretórios potenciais a serem verificados. Em seguida, realizou-se a enumeração com o **Gobuster** usando `gobuster dir -u http://54.152.245.201:80 -w /home/kali/investigations/acme-corp/custom_wordlist.txt -o /home/kali/investigations/acme-corp/gobuster_results.txt -q`. Os resultados foram gravados em [gobuster_results.txt](./acme-corp/gobuster_results.txt). A imagem 15 exibe os achados obtidos durante essa varredura.

<div align="center"><figure>
    <img src="../0-aux/md3-img15.png" alt="img15"><br>
    <figcaption>Imagem 15.</figcaption>
</figure></div><br>

Três diretórios foram encontrados: `wp-admin`, `wp-content` e `wp-includes`. Cada um correspondia a um endpoint acessível tanto via **Curl** quanto pelo navegador. No navegador foram abertos `http://54.152.245.201:80/wp-admin`, `http://54.152.245.201:80/wp-content` e `http://54.152.245.201:80/wp-includes`. O diretório `wp-content` não revelou conteúdo relevante, enquanto `wp-includes` e `wp-admin` expuseram diversas informações sobre a estrutura interna do **WordPress**, facilitando a identificação de rotas, arquivos e potenciais pontos de interesse para exploração. As imagens 16 e 17 exibem o acesso a essas páginas.

<div align="center"><figure>
    <img src="../0-aux/md3-img16.png" alt="img16"><br>
    <figcaption>Imagem 16.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img17.png" alt="img17"><br>
    <figcaption>Imagem 17.</figcaption>
</figure></div><br>

🔍 Fase 5 — Análise   
Na fase cinco, uma análise avançada foi realizado com a ferramenta **SpiderFoot** que foi implantada em um dos containers. A interface web foi acessada pelo navegador da máquina física **Windows** usando o IP ou DNS público da instância na porta `5001`. Para permitir esse acesso, foi criada uma regra no security group da instância liberando a porta `5001` apenas para o IP público da máquina física, e o acesso ao container só foi possível porque o **Docker Compose** estava configurado com o mapeamento de portas `5001:5001`. A imagem 18 evidencia o acesso ao **SpiderFoot**.

<div align="center"><figure>
    <img src="../0-aux/md3-img18.png" alt="img18"><br>
    <figcaption>Imagem 18.</figcaption>
</figure></div><br>

O passo seguinte foi criar um scan para o domínio `acme-corp-lab.com`, selecionando módulos relevantes para DNS, HTTP e outras fontes de dados. Assim, o **SpiderFoot** coletava dados de fontes que não foi possível acessar manualmente. A imagem 19 mostra....

<div align="center"><figure>
    <img src="../0-aux/md3-img19.png" alt="img19"><br>
    <figcaption>Imagem 19.</figcaption>
</figure></div><br>

📊 Fase 6 — Documentação   
A fase final deste laboratório consistiu em registrar todas as descobertas. Foi criado o arquivo em **Markdown** [investigation_report.md](./acme-corp/investigation_report.md), contendo o relatório detalhado das informações obtidas durante o lab. Além disso, gerou-se o arquivo [target_ips.txt](./acme-corp/target_ips.txt), com o mapeamento dos IPs descobertos, que serviria como referência para o próximo laboratório. Ambos os arquivos foram armazenados no diretório `acme-corp`.

Neste laboratório, o processo de OSINT foi conduzido principalmente pelo container de ataque **Kali Linux** no ambiente **Docker**, recorrendo ao navegador em alguns casos para melhor visualização das informações. Todos os comandos executados no Kali tiveram seus outputs salvos em arquivos — uma prática recomendada para documentação. Para organizar esses arquivos, foi criada a pasta `acme-corp` dentro do diretório `investigations`. Esse diretório poderia, em outros laboratórios, abrigar pastas de investigação de sistemas de outras empresas. Por enquanto, trabalhou-se apenas com a Acme Corp. O diretório de investigação foi localizado em `/home/kali` dentro do container.

A mesma estrutura foi replicada na pasta do módulo, criando o diretório [acme-corp](./acme-corp/) e copiando para ele todos os arquivos gerados no container. A seguir está a estrutura de arquivos com uma breve descrição de cada item:
- [acme-corp](./acme-corp/):
  - [amass_results.txt](./acme-corp/amass_results.txt): resultados da enumeração do **Amass**.
  - [company_info.txt](./acme-corp/company_info.txt): arquivo obtido do bucket público do **Amazon S3**, contendo informações sobre a empresa, infraestrutura e servidores (e‑mail e DNS).
  - [custom_wordlist.txt](./acme-corp/custom_wordlist.txt): wordlist personalizada usada pelo **Gobuster**.
  - [employees.csv](./acme-corp/employees.csv): arquivo baixado do mesmo bucket público do **Amazon S3**, contendo uma lista de funcionários com endereços de e‑mail e números de telefone.
  - [gobuster_results.txt](./acme-corp/gobuster_results.txt): resultados da enumeração de diretórios pelo **Gobuster**.
  - [investigation_report.md](./acme-corp/investigation_report.md): relatório final em Markdown com as descobertas do lab.
  - [legacy_page.html](./acme-corp/legacy_page.html): cópia da página final obtida a partir do redirecionamento do subdomínio `old`.
  - [live_web_services.txt](./acme-corp/live_web_services.txt): saída do **httpx** com serviços web ativos (títulos, tecnologias e status).
  - [old_acme_page.html](./acme-corp/old_acme_page.html): cópia da página original de `old.acme-corp-lab.com` — idêntica a `legacy_page.html`, pois redirecionava para essa página.
  - [resolved_subdomains.txt](./acme-corp/resolved_subdomains.txt): subdomínios que resolveram em registros DNS válidos (saída do **dnsx**).
  - [subdomains.txt](./acme-corp/subdomains.txt): lista de subdomínios identificados com o **Subfinder**.
  - [target_ips.txt](./acme-corp/target_ips.txt): mapeamento dos IPs descobertos, usado como referência para labs subsequentes.

<a name="item3.2"><h4>3.2 Lab 2: Descobrindo os Segredos dos Servidores Web</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_2)

Obs.: Laboratório registrado como 2, documento como 2 e referente a aula 2.

<details><summary><strong>Ambiente de Laboratório</strong></summary>
  <ul>
    <li><details><summary><strong>Docker Compose</strong></summary>
        <ul>
          <li><details><summary><strong>services:</strong></summary>
            <ul>
              <li><details><summary><strong>kali:</strong></summary>
                <ul>
                  <li><strong>build:</strong>
                    <ul>
                      <li><code>context: .</code>: Contexto da build é o diretório atual.</li>
                      <li><code>dockerfile: kali.Dockerfile</code>: Usa o arquivo <code>kali.Dockerfile</code> para construir a imagem.</li>
                    </ul>
                  </li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_kali</code>.</li>
                  <li><code>tty: true</code>: Permite alocar um terminal interativo para o container.</li>
                  <li><code>stdin_open: true</code>: Mantém o STDIN aberto para uso com <code>docker exec -it</code>.</li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./labs:/home/kali/labs</code>: Monta o diretório local <code>./labs</code> dentro do container em <code>/home/kali/labs</code>, facilitando desenvolvimento e uso de scripts.</li>
                    </ul>
                  </li>
                  <li><strong>command:</strong> <code>/bin/bash</code>: Inicia o shell Bash, mantendo o container pronto para uso.</li>
                </ul>
              </details></li>
              <li><details><summary><strong>spiderfoot:</strong></summary>
                <ul>
                  <li><strong>build:</strong>
                    <ul>
                      <li><code>context: .</code>: Define o diretório atual como contexto da build.</li>
                      <li><code>dockerfile: Dockerfile.spiderfoot</code>: Usa o Dockerfile dedicado do SpiderFoot para construir a imagem.</li>
                    </ul>
                  </li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_spiderfoot</code>.</li>
                  <li><strong>ports:</strong>
                    <ul>
                      <li><code>"5001:5001"</code>: Expõe a interface web do SpiderFoot na porta 5001 do host.</li>
                    </ul>
                  </li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./spiderfoot-data:/root/.spiderfoot</code>: Monta o diretório local <code>./spiderfoot-data</code> dentro do container em <code>/root/.spiderfoot</code>, garantindo que configurações, resultados e dados do SpiderFoot sejam persistidos entre reinicializações do container.</li>
                    </ul>
                  </li>
                </ul>
              </details></li>
              <li><details><summary><strong>neo4j:</strong></summary>
                <ul>
                  <li><strong>image:</strong> Usa a imagem oficial <code>neo4j:4.4</code>.</li>
                  <li><strong>container_name:</strong> Define o nome do container como <code>kensei_neo4j</code>.</li>
                  <li><strong>environment:</strong>
                    <ul>
                      <li><code>NEO4J_AUTH=neo4j/test</code>: Define usuário e senha iniciais (<code>neo4j</code> / <code>test</code>).</li>
                      <li><code>NEO4J_dbms_memory_heap_initial__size=512m</code>: Define o tamanho inicial do heap de memória do Neo4j como 512 MB.</li>
                      <li><code>NEO4J_dbms_memory_heap_max__size=1g</code>: Define o tamanho máximo do heap de memória do Neo4j como 1 GB.</li>
                    </ul>
                  </li>
                  <li><strong>ports:</strong>
                    <ul>
                      <li><code>"7474:7474"</code>: Mapeia a porta 7474 do container para a porta 7474 do host, permitindo acesso à interface web do Neo4j via navegador.</li>
                      <li><code>"7687:7687"</code>: Mapeia a porta 7687 do container para a porta 7687 do host, utilizada pelo Bolt protocol para conexão de drivers e aplicações ao Neo4j.</li>
                    </ul>
                  </li>
                  <li><strong>volumes:</strong>
                    <ul>
                      <li><code>- ./neo4j-data:/data</code>: Monta o diretório local <code>./neo4j-data</code> dentro do container em <code>/data</code>, garantindo que o banco de dados Neo4j seja persistido no host entre reinicializações do container.</li>
                    </ul>
                  </li>
                </ul>
              </details></li>
            </ul>
          </details></li>
          <li><details><summary><strong>networks:</strong></summary>
            <ul>
              <li><code>default:</code>
                <ul>
                  <li><strong>name:</strong> <code>kensei_lab_net</code>: Nomeia a rede padrão usada pelos serviços, garantindo comunicação entre containers dentro dessa rede.</li>
                </ul>
              </li>
            </ul>
          </details></li>
        </ul>
      </details></li>
    <li><details><summary><strong>Dockerfile</strong></summary>
      <ul>
        <li><details><summary><strong>Dockerfile.kali</strong></summary>
          <ul>
            <li><code>FROM kalilinux/kali-rolling</code>: Imagem base Kali Rolling.</li>
            <li><strong>RUN:</strong> Executa a instalação de ferramentas essenciais e limpa o cache para reduzir o tamanho da imagem:
              <ul>
                <li><code>apt-get update</code>: Atualiza a lista de pacotes disponíveis.</li>
                <li><code>apt-get install -y git curl jq python3-pip build-essential golang-go amass</code>: Instala ferramentas essenciais de desenvolvimento, rede e pentest.</li>
                <li><code>apt-get clean && rm -rf /var/lib/apt/lists/*</code>: Remove arquivos temporários e limpa cache do apt.</li>
              </ul>
            </li>
            <li><code>ENV GOPATH=/root/go</code>: Define a variável <code>GOPATH</code> apontando para <code>/root/go</code>, diretório onde o Go instalará pacotes e binários do usuário.</li>
            <li><code>ENV PATH=$PATH:/root/go/bin</code>: Adiciona <code>/root/go/bin</code> ao <code>PATH</code>, permitindo executar ferramentas instaladas via <code>go install</code>.</li>
            <li><code>RUN mkdir -p $GOPATH</code>: Cria diretório do GOPATH.</li>
            <li><strong>RUN:</strong> Instala ferramentas Go no <code>GOPATH</code> utilizando <code>go install ...@latest</code> e disponibiliza os binários em <code>/root/go/bin</code>:
              <ul>
                <li><code>go install github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest</code>: Instala o <strong>subfinder</strong>, usado para descoberta de subdomínios.</li>
                <li><code>go install github.com/projectdiscovery/httpx/cmd/httpx@latest</code>: Instala o <strong>httpx</strong>, utilizado para verificação e coleta de informações HTTP (status, headers, títulos, etc.).</li>
                <li><code>go install github.com/projectdiscovery/nuclei/v2/cmd/nuclei@latest</code>: Instala o <strong>nuclei</strong>, motor de scan de vulnerabilidades baseado em templates.</li>
                <li><code>go install github.com/hakluke/hakrawler@latest</code>: Instala o <strong>hakrawler</strong>, um crawler rápido para descoberta de endpoints e conteúdo web.</li>
                <li><code>go install github.com/zricethezav/gitleaks/v8@latest</code>: Instala o <strong>gitleaks</strong>, ferramenta para detecção de segredos e chaves em repositórios.</li>
              </ul>
            </li>
            <li><code>COPY scripts /home/kali/scripts</code>: Copia scripts locais para dentro do container.</li>
            <li><code>WORKDIR /home/kali</code>: Define diretório de trabalho.</li>
            <li><code>CMD ["/bin/bash"]</code>: Inicia o shell Bash.</li>
          </ul>
        </details></li>
        <li><details><summary><strong>Dockerfile.spiderfoot</strong></summary>
          <ul>
            <li><code>FROM python:3.10-slim</code>: Usa a imagem base Python 3.10 slim como ambiente inicial.</li>
            <li><strong>RUN</strong>:
              <ul>
                <li><code>apt-get update</code>: Atualiza a lista de pacotes.</li>
                <li><code>apt-get install -y --no-install-recommends git build-essential python3-dev libyaml-dev curl ca-certificates gcc g++ make libffi-dev libssl-dev</code>: Instala dependências do sistema necessárias para compilar e executar as dependências Python do SpiderFoot.</li>
                <li><code>rm -rf /var/lib/apt/lists/*</code>: Remove listas de pacotes para reduzir o tamanho final da imagem.</li>
              </ul>
            </li>
            <li><code>RUN pip3 install -U pip setuptools wheel cython</code>: Atualiza o gerenciador de pacotes Python e instala ferramentas de build (necessárias para compilar extensões como PyYAML).</li>
            <li><code>WORKDIR /app</code>: Define o diretório de trabalho para os comandos seguintes.</li>
            <li><code>RUN git clone --depth=1 --branch v4.0 https://github.com/smicallef/spiderfoot /app</code>: Clona a versão 4.0 do repositório SpiderFoot diretamente para <code>/app</code>.</li>
            <li><code>RUN sed -i 's/pyyaml>=5.4.1,<6/pyyaml>=6.0/' requirements.txt</code>: Altera a especificação do PyYAML no <code>requirements.txt</code> para uma versão compatível (quando necessário) antes da instalação das dependências.</li>
            <li><code>RUN pip3 install -r requirements.txt</code>: Instala as dependências Python do SpiderFoot listadas em <code>requirements.txt</code>.</li>
            <li><code>EXPOSE 5001</code>: Expõe a porta 5001, usada pela interface web do SpiderFoot.</li>
            <li><code>CMD ["python3", "sf.py", "-l", "0.0.0.0:5001"]</code>: Comando padrão para iniciar o SpiderFoot, fazendo-o escutar em todas as interfaces na porta 5001.</li>
          </ul>
        </details></li>
      </ul>
    </details></li>
  </ul>
</details>



Este segundo laboratório foi sequência do laboratório anterior e montou o ambiente conforme tinha sido finalizado o lab anterior. Ou seja, os arquivos já criados estavam todos no container e o mesmo diretótio, `acme-corp`, seria utilizado para armazenar novos arquivos gerados. O ambiente **Docker** teve algumas alterações que não influenciavam a execução ou continuação do lab. Apenas o container **Kali Linux** era utilizado nesse lab, enquanto os demais não foram implantados pois não era necessários.

Com a execução do OSINT (footprinting passivo e ativo, de forma não agressiva) foram coletadas apenas informações públicas. Neste lab, aprofundou-se a investigação no sistema da Acme Corp para identificar serviços e vetores potenciais — etapa que marca o limiar entre ações permitidas e não permitidas em ambientes reais. Em produção, qualquer verificação intrusiva exige autorização do responsável pelo sistema; neste caso, por se tratar de um laboratório controlado, a continuação foi autorizada pelos instrutores do curso.

Antes de executar os comandos, acessou-se o container de ataque com `docker exec -it kensei_kali /bin/bash` e navegou-se até a pasta de investigação da Acme Corp com `cd /home/kali/investigations/acme-corp`, onde já estavam os arquivos gerados durante o primeiro lab. Para confirmar a existência do arquivo `target_ips.txt`, executou-se `ls -la target_ips.txt`; se nada fosse listado, o arquivo não existia e era necessário criar um com os três IPs dos servidores identificados e o domínio `www.acme-corp-lab.com`.

Em seguida, dentro de `acme-corp` criou-se o diretório [results](./acme-corp/results/) e, nele, uma pasta para cada alvo, nomeada com o IP ou domínio correspondente. Para simplificar a organização e evitar nomes longos, adicionou-se ao `target_ips.txt` um mapeamento que relacionava cada IP/domínio a uma nomenclatura padrão e sucinta (ex.: `ip1`, `ip2`, ...). A criação de pastas, subpastas e arquivos foi baseada nesse mapeamento. A estrutura ficou assim:
- [acme-corp](./acme-corp/):
  - [results](./acme-corp/results/) (`mkdir -p results`):
    - [ip1](./acme-corp/results/ip1/) (`mkdir -p results/ip1`): `www.acme-corp-lab.com` — domínio do site principal.
    - [ip2](./acme-corp/results/ip2/) (`mkdir -p results/ip2`): `3.94.82.59` — IP do servidor do sistema Legacy.
    - [ip3](./acme-corp/results/ip3/) (`mkdir -p results/ip3`): `34.207.53.34` — IP do servidor de desenvolvimento.
    - [ip4](./acme-corp/results/ip4/) (`mkdir -p results/ip4`): `54.152.245.201` — IP do servidor de administração.

O arquivo [target_ips.txt](./acme-corp/target_ips.txt) trazia o mapeamento dos três IPs dos servidores descobertos, além do domínio do site principal. Esse arquivo serviu de base para um reconhecimento mais agressivo — ou seja, atividades com maior interação sobre os alvos, passíveis de detecção por sistemas de defesa. Para cada IP realizou-se varredura de portas (identificação de portas abertas, serviços, versões, banners e possível sistema operacional), enumeração de diretórios e análise dos endpoints de API. Todas as novas informações foram registradas no relatório existente ([investigation_report.md](./acme-corp/investigation_report.md)), e os arquivos gerados foram salvos na pasta correspondente a cada alvo.

🚀 Passo 1 - Reconhecimento Rápido de Portas   
Para o scan de portas foram utilizado três estratégias. A primeira delas com o **Nmap** para uma varredura das 1.000 portas mais comuns (rápido e eficiente). A estratégia 2 foi utilizando o **Rustscan** para descoberta rápida de portas não-padrão/altas. Enquanto a última foi novamente com o **Nmap** focando em portas web comuns, útil para pentesting de aplicações web. Abaixo seguem os comandos executados, separados por alvo:
- **3.94.82.59 (ip2)**:
  - Estratégia 1: `nmap -sT --top-ports 1000 3.94.82.59 -oN results/ip2/nmap_top1000_ip2.txt`
  - Estratégia 2: `rustscan -a 3.94.82.59 --ulimit 5000 -- -sT -T4 -oA results/ip2/rustscan_ip2.txt`
  - Estratégia 3: `nmap -sT -T4 -p 80,443,3000,8080,8443,8000,9000 3.94.82.59 -oN results/ip2/nmap_web_ip2.txt`
- **34.207.53.34 (ip3)**:
  - Estratégia 1: `nmap -sT --top-ports 1000 34.207.53.34 -oN results/ip3/nmap_top1000_ip3.txt`
  - Estratégia 2: `rustscan -a 34.207.53.34 --ulimit 5000 -- -sT -T4 -oA results/ip3/rustscan_ip3.txt`
  - Estratégia 3: `nmap -sT -T4 -p 80,443,3000,8080,8443,8000,9000 34.207.53.34 -oN results/ip3/nmap_web_ip3.txt`
- **54.152.245.201 (ip4)**:
  - Estratégia 1: `nmap -sT --top-ports 1000 54.152.245.201 -oN results/ip4/nmap_top1000_ip4.txt`
  - Estratégia 2: `rustscan -a 54.152.245.201 --ulimit 5000 -- -sT -T4 -oA results/ip4/rustscan_ip4.txt`
  - Estratégia 3: `nmap -sT -T4 -p 80,443,3000,8080,8443,8000,9000 54.152.245.201 -oN results/ip4/nmap_web_ip4.txt`

<div align="center"><figure>
    <img src="../0-aux/md3-img20.png" alt="img20"><br>
    <figcaption>Imagem 20.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img25.png" alt="img25"><br>
    <figcaption>Imagem 25.</figcaption>
</figure></div><br>

Cada comando gerou um arquivo — como foram três comandos por alvo e quatro alvos, isso resultou em 12 arquivos no total. Para visualizar o conteúdo de qualquer um deles, foi utilizado o **Cat** passando o caminho para o arquivo. A seguir está a lista dos arquivos gerados, os comandos para visualização e os principais resultados obtidos:
- **3.94.82.59 (ip2)**:
  - `nmap_top1000_ip2.txt` (`cat results/ip2/nmap_top1000_ip2.txt`): 
  - `rustscan_ip2.txt` (`cat results/ip2/rustscan_ip2.txt`): 
  - `nmap_web_ip2.txt` (`cat results/ip2/nmap_web_ip2.txt`): 
- **34.207.53.34 (ip3)**:
  - `nmap_top1000_ip3.txt` (`cat results/ip3/nmap_top1000_ip3.txt`): 
  - `rustscan_ip3.txt` (`cat results/ip3/rustscan_ip3.txt`): 
  - `nmap_web_ip3.txt` (`cat results/ip3/nmap_web_ip3.txt`): 
- **54.152.245.201 (ip4)**:
  - `nmap_top1000_ip4.txt` (`cat results/ip4/nmap_top1000_ip4.txt`):
  - `rustscan_ip4.txt` (`cat results/ip4/rustscan_ip4.txt`):
  - `nmap_web_ip4.txt` (`cat results/ip4/nmap_web_ip4.txt`):


🔍 Passo 2 — Scan Detalhado de Serviços   
Com as portas abertas e serviços identificados, o próximo passo foi executar varreduras detalhadas nessas portas para obter informações mais precisas e potenciais vetores de exploração. Para isso utilizou-se o **Nmap** com scripts e detecção de versão: `nmap -sC -sV -p <portas> <alvo>`, onde `-sC` executava os scripts NSE padrão (verificação de banners, checks rápidos e detecção de problemas conhecidos) e `-sV` tenta identificar versões exatas dos serviços em execução. Os resultados permitiram confirmar serviços, versões e possíveis vulnerabilidades a serem investigadas nas etapas subsequentes. A lista abaixo exibe os comandos executados:
- **3.94.82.59 (ip2)**  
  - `nmap -sC -sV -p <ports> 3.94.82.59 -oN results/ip2/nmap_ip2_port.txt`
- **34.207.53.34 (ip3)**  
  - `nmap -sC -sV -p <ports> 34.207.53.34 -oN results/ip3/nmap_ip3_port.txt`
- **54.152.245.201 (ip4)**  
  - `nmap -sC -sV -p <ports> 54.152.245.201 -oN results/ip4/nmap_ip4_port.txt`

<div align="center"><figure>
    <img src="../0-aux/md3-img26.png" alt="img26"><br>
    <figcaption>Imagem 26.</figcaption>
</figure></div><br>


<div align="center"><figure>
    <img src="../0-aux/md3-img30.png" alt="img30"><br>
    <figcaption>Imagem 30.</figcaption>
</figure></div><br>

Novamente, cada comando gerou um arquivo — agora com resultado por alvo e pelas portas especificadas. A seguir está a lista dos arquivos gerados, o comando para visualização e um resumo dos principais achados extraídos desses arquivos:
- **3.94.82.59 (ip2)**  
  - `nmap_ip2_port.txt` (`cat results/ip2/nmap_ip2_port.txt`): 
- **34.207.53.34 (ip3)**  
  - `nmap_ip3_port.txt` (`cat results/ip3/nmap_ip3_port.txt`): 
- **54.152.245.201 (ip4)**  
  - `nmap_ip4_port.txt` (`cat results/ip4/nmap_ip4_port.txt`): 

🌐 Passo 3 - Análise Detalhada de Serviços HTTP/HTTPS   
Nessa etapa, o foco foi nos serviços web identificados nas varreduras anteriores. Com as portas e serviços mapeados, separaram-se os servidores que estavam executando serviços HTTP/HTTPS e foram usadas ferramentas para identificar tecnologias, títulos e códigos de status, além de obter impressões mais detalhadas sobre frameworks e versões. Utilizou-se **httpx** para detecção rápida de tecnologias (PHP, Node.js, etc.), título da página e código HTTP, e **whatweb** para uma fingerprinting mais aprofundada (versões de CMS, frameworks e bibliotecas). A lista abaixo mostra os comandos executados por alvo:
- **3.94.82.59 (ip2)**  
  - `httpx -u http://3.94.82.59 -title -tech-detect -status-code -o results/ip2/httpx_ip2.txt`
  - `whatweb -a 3 http://3.94.82.59 > results/ip2/whatweb_ip2.txt`
- **34.207.53.34 (ip3)**  
  - `httpx -u http://34.207.53.34 -title -tech-detect -status-code -o results/ip3/httpx_ip3.txt`
  - `whatweb -a 3 http://34.207.53.34 > results/ip3/whatweb_ip3.txt`
- **54.152.245.201 (ip4)**  
  - `httpx -u http://54.152.245.201 -title -tech-detect -status-code -o results/ip4/httpx_ip4.txt`
  - `whatweb -a 3 http://54.152.245.201 > results/ip4/whatweb_ip4.txt`

A seguir apresenta-se a lista dos arquivos gerados por esses comandos, o comando para visualização de cada um e um resumo das principais informações extraídas de cada arquivo:
- **3.94.82.59 (ip2)**  
  - `httpx_ip2.txt` (`cat results/ip2/httpx_ip2.txt`): 
  - `whatweb_ip2.txt` (`cat results/ip2/whatweb_ip2.txt`): 
- **34.207.53.34 (ip3)**  
  - `httpx_ip3.txt` (`cat results/ip3/httpx_ip3.txt`): 
  - `whatweb_ip3.txt` (`cat results/ip3/whatweb_ip3.txt`): 
- **54.152.245.201 (ip4)**  
  - `httpx_ip4.txt` (`cat results/ip4/httpx_ip4.txt`): 
  - `whatweb_ip4.txt` (`cat results/ip4/whatweb_ip4.txt`): 

<div align="center"><figure>
    <img src="../0-aux/md3-img31.png" alt="img31"><br>
    <figcaption>Imagem 31.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img35.png" alt="img35"><br>
    <figcaption>Imagem 35.</figcaption>
</figure></div><br>

📁 Passo 4 — Enumeração de Diretórios e Arquivos   
Aplicações web frequentemente expõem diretórios e arquivos sensíveis (backups, logs, arquivos de configuração e até código‑fonte). Nesta etapa realizou‑se a enumeração de diretórios e arquivos com **Gobuster**, que permite descobrir caminhos acessíveis diretamente, mas não listados publicamente. Foram adotadas três abordagens complementares: (1) uso da wordlist padrão do **Gobuster**; (2) uso de uma wordlist personalizada; (3) busca por extensões específicas com o parâmetro `-x` para localizar arquivos como `.conf`, `.ini`, `.bak`, `.htaccess`, `robots.txt` e `sitemap.xml`, que frequentemente contêm dados sensíveis.

Como já havia uma `custom_wordlist.txt` criada em etapa anterior, gerou‑se uma segunda wordlist (para evitar sobrescrever a original) chamada `custom_wordlist_extended.txt` com:  
`echo -e 'admin\napi\nbackup\nconfig\ndev\ngit\nlogin\nphpinfo\nphpmyadmin\ntest\nwww\nwp-admin\nwp-content\nwp-includes\nuploads\nfiles\nimages\ncss\njs\nassets\n.htaccess\nrobots.txt\nsitemap.xml' > custom_wordlist_extended.txt`.

A seguir, os comandos executados por alvo são listados:
- **3.94.82.59 (ip2)**  
  - `gobuster dir -u http://3.94.82.59 -w /usr/share/wordlists/dirb/common.txt -o results/ip2/gobuster_ip2.txt -q`
  - `gobuster dir -u http://3.94.82.59 -w custom_wordlist.txt -o results/ip2/gobuster_custom_ip2.txt -q`
  - `gobuster dir -u http://3.94.82.59 -w /usr/share/wordlists/dirb/common.txt -x php,html,txt,conf,ini,bak,log -o results/ip2/gobuster_files_ip2.txt -q`
- **34.207.53.34 (ip3)**  
  - `gobuster dir -u http://34.207.53.34 -w /usr/share/wordlists/dirb/common.txt -o results/ip3/gobuster_ip3.txt -q`
  - `gobuster dir -u http://34.207.53.34 -w custom_wordlist.txt -o results/ip3/gobuster_custom_ip3.txt -q`
  - `gobuster dir -u http://34.207.53.34 -w /usr/share/wordlists/dirb/common.txt -x php,html,txt,conf,ini,bak,log -o results/ip3/gobuster_files_ip3.txt -q`
- **54.152.245.201 (ip4)**  
  - `gobuster dir -u http://54.152.245.201 -w /usr/share/wordlists/dirb/common.txt -o results/ip4/gobuster_ip4.txt -q`
  - `gobuster dir -u http://54.152.245.201 -w custom_wordlist.txt -o results/ip4/gobuster_custom_ip4.txt -q`
  - `gobuster dir -u http://54.152.245.201 -w /usr/share/wordlists/dirb/common.txt -x php,html,txt,conf,ini,bak,log -o results/ip4/gobuster_files_ip4.txt -q`

Essas abordagens permitiram mapear diretórios públicos, localizar arquivos úteis para investigação e identificar potenciais fontes de credenciais e configurações expostas. Como de costume, cada comando gerava um arquivo diferente. A lista abaixo mostra os arquivos gerados o comando para visualização de cada um e um resumo das principais informações extraídas de cada arquivo:
- **3.94.82.59 (ip2)**  
  - `gobuster_ip2.txt` (`cat results/ip2/gobuster_ip2.txt`): 
  - `gobuster_custom_ip2.txt` (`cat results/ip2/gobuster_custom_ip2.txt`): 
  - `gobuster_files_ip2.txt` (`cat results/ip2/gobuster_files_ip2.txt`): 
- **34.207.53.34 (ip3)**  
  - `gobuster_ip3.txt` (`cat results/ip3/gobuster_ip3.txt`): 
  - `gobuster_custom_ip3.txt` (`cat results/ip3/gobuster_custom_ip3.txt`): 
  - `gobuster_files_ip3.txt` (`cat results/ip3/gobuster_files_ip3.txt`): 
- **54.152.245.201 (ip4)**  
  - `gobuster_ip4.txt` (`cat results/ip4/gobuster_ip4.txt`): 
  - `gobuster_custom_ip4.txt` (`cat results/ip4/gobuster_custom_ip4.txt`): 
  - `gobuster_files_ip4.txt` (`cat results/ip4/gobuster_files_ip4.txt`): 

<div align="center"><figure>
    <img src="../0-aux/md3-img36.png" alt="img36"><br>
    <figcaption>Imagem 36.</figcaption>
</figure></div><br>

<div align="center"><figure>
    <img src="../0-aux/md3-img40.png" alt="img40"><br>
    <figcaption>Imagem 40.</figcaption>
</figure></div><br>















<a name="item3.3"><h4>3.3 Firewall & ACL</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_3)






<a name="item3.4"><h4>3.4 IDS e IPS</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking)

A aula 4 não teve nenhum laboratório ou o laboratório ainda não foi construído pelo professor do curso.


<a name="item3.5"><h4>3.5 Monitoramento de Logs</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking)

A aula 5 não teve nenhum laboratório ou o laboratório ainda não foi construído pelo professor do curso.

<a name="item3.6"><h4>3.6 Patch Management</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_4)






<a name="item3.7"><h4>3.7 Cloud Security</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_7)

A aula 7 não teve nenhum laboratório ou o laboratório ainda não foi construído pelo professor do curso.

<a name="item3.8"><h4>3.8 IAM e Permissionamento</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_8)

A aula 8 não teve nenhum laboratório ou o laboratório ainda não foi construído pelo professor do curso.


<a name="item3.9"><h4>3.9 Container Security Docker Bench & Trivy</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_5)








<a name="item3.10"><h4>3.10 NIST & Resposta a Incidentes</h4></a>[Back to summary](#item3)   
[Material do Lab](https://github.com/Kensei-CyberSec-Lab/formacao-cybersec/tree/main/modulo3-ethical-hacking/lab_6)
